Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Jin2021a,
abstract = {Unmanned aerial vehicles are rapidly gaining popularity in a variety of environmental monitoring tasks. A key requirement for their autonomous operation is the ability to perform efficient environmental mapping online, given limited onboard resources constraining operation time, travel distance, and computational capacity. To address this, we present an online adaptive-resolution approach for mapping terrain based on Gaussian Process fusion. A key aspect of our approach is an integral kernel encoding spatial correlation over the areas of grid cells, which enables modifying map resolution while maintaining correlations in a theoretically sound fashion. This way, we can retain details in areas of interest at higher map resolutions while compressing information in uninteresting areas at coarser resolutions to achieve a compact map representation of the environment. We evaluate the performance of our approach on both synthetic and real-world data. Results show that our method is more efficient in terms of mapping time and memory consumption without compromising on map quality. Finally, we integrate our mapping strategy into an adaptive path planning framework to show that it facilitates information gathering efficiency in online settings.},
archivePrefix = {arXiv},
arxivId = {2109.14257},
author = {Jin, Liren and R{\"{u}}ckin, Julius and Kiss, Stefan H. and Vidal-Calleja, Teresa and Popovi{\'{c}}, Marija},
eprint = {2109.14257},
file = {:home/gian/Documents/Lonomy/ST/Literature/Adaptive-Resolution Field Mapping.pdf:pdf},
pages = {1--8},
title = {{Adaptive-Resolution Field Mapping Using Gaussian Process Fusion with Integral Kernels}},
url = {http://arxiv.org/abs/2109.14257},
year = {2021}
}

@article{Miki2022,
abstract = {Perceiving the surrounding environment is crucial for autonomous mobile robots. An elevation map provides a memory-efficient and simple yet powerful geometric representation for ground robots. The robots can use this information for navigation in an unknown environment or perceptive locomotion control over rough terrain. Depending on the application, various post processing steps may be incorporated, such as smoothing, inpainting or plane segmentation. In this work, we present an elevation mapping pipeline leveraging GPU for fast and efficient processing with additional features both for navigation and locomotion. We demonstrated our mapping framework through extensive hardware experiments. Our mapping software was successfully deployed for underground exploration during DARPA Subterranean Challenge and for various experiments of quadrupedal locomotion.},
archivePrefix = {arXiv},
arxivId = {2204.12876},
author = {Miki, Takahiro and Wellhausen, Lorenz and Grandia, Ruben and Jenelten, Fabian and Homberger, Timon and Hutter, Marco},
eprint = {2204.12876},
file = {:home/gian/Documents/Lonomy/ST/Elevation Mapping for Locomotion and Navigation using GPU.pdf:pdf},
title = {{Elevation Mapping for Locomotion and Navigation using GPU}},
url = {http://arxiv.org/abs/2204.12876},
year = {2022}
}
@article{GhaffariJadidi2018a,
abstract = {Most of the existing robotic exploration schemes use occupancy grid representations and geometric targets known as frontiers. The occupancy grid representation relies on the assumption of independence between grid cells and ignores structural correlations present in the environment. We develop a Gaussian processes (GPs) occupancy mapping technique that is computationally tractable for online map building due to its incremental formulation and provides a continuous model of uncertainty over the map spatial coordinates. The standard way to represent geometric frontiers extracted from occupancy maps is to assign binary values to each grid cell. We extend this notion to novel probabilistic frontier maps computed efficiently using the gradient of the GP occupancy map. We also propose a mutual information-based greedy exploration technique built on that representation that takes into account all possible future observations. A major advantage of high-dimensional map inference is the fact that such techniques require fewer observations, leading to a faster map entropy reduction during exploration for map building scenarios. Evaluations using the publicly available datasets show the effectiveness of the proposed framework for robotic mapping and exploration tasks.},
archivePrefix = {arXiv},
arxivId = {1605.00335},
author = {{Ghaffari Jadidi}, Maani and {Valls Miro}, Jaime and Dissanayake, Gamini},
doi = {10.1007/s10514-017-9668-3},
eprint = {1605.00335},
file = {:home/gian/Documents/Lonomy/ST/Literature/Gaussian Process Autonomous Mapping and Exploration.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Autonomous navigation,Exploration,Gaussian processes,Mapping,Mutual information},
number = {2},
pages = {273--290},
title = {{Gaussian processes autonomous mapping and exploration for range-sensing mobile robots}},
volume = {42},
year = {2018}
}
@article{Kochanov2016a,
abstract = {Scene understanding is an important prerequisite for vehicles and robots that operate autonomously in dynamic urban street scenes. For navigation and high-level behavior planning, the robots not only require a persistent 3D model of the static surroundings-equally important, they need to perceive and keep track of dynamic objects. In this paper, we propose a method that incrementally fuses stereo frame observations into temporally consistent semantic 3D maps. In contrast to previous work, our approach uses scene flow to propagate dynamic objects within the map. Our method provides a persistent 3D occupancy as well as semantic belief on static as well as moving objects. This allows for advanced reasoning on objects despite noisy single-frame observations and occlusions. We develop a novel approach to discover object instances based on the temporally consistent shape, appearance, motion, and semantic cues in our maps. We evaluate our approaches to dynamic semantic mapping and object discovery on the popular KITTI benchmark and demonstrate improved results compared to single-frame methods.},
author = {Kochanov, Deyvid and O{\v{s}}ep, Aljo{\v{s}}a and St{\"{u}}ckler, J{\"{o}}rg and Leibe, Bastian},
doi = {10.1109/IROS.2016.7759285},
file = {:home/gian/Documents/Lonomy/ST/Literature/Scene Flow Propagation for Semantic Mapping and Object Discovery in.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {3},
pages = {1785--1792},
title = {{Scene flow propagation for semantic mapping and object discovery in dynamic street scenes}},
volume = {2016-Novem},
year = {2016}
}

@article{Zobeidi2020a,
abstract = {We develop an online probabilistic metric-semantic mapping approach for autonomous robots relying on streaming RGB-D observations. We cast this problem as a Bayesian inference task, requiring encoding both the geometric surfaces and semantic labels (e.g., chair, table, wall) of the unknown environment. We propose an online Gaussian Process (GP) training and inference approach, which avoids the complexity of GP classification by regressing a truncated signed distance function representation of the regions occupied by different semantic classes. Online regression is enabled through sparse GP approximation, compressing the training data to a finite set of inducing points, and through spatial domain partitioning into an Octree data structure with overlapping leaves. Our experiments demonstrate the effectiveness of this technique for large-scale probabilistic metric-semantic mapping of 3D environments. A distinguishing feature of our approach is that the generated maps contain full continuous distributional information about the geometric surfaces and semantic labels, making them appropriate for uncertainty-aware planning.},
author = {Zobeidi, Ehsan and Koppel, Alec and Atanasov, Nikolay},
doi = {10.1109/IROS45743.2020.9341658},
file = {:home/gian/Documents/Lonomy/ST/Literature/Dense_Incremental_Metric-Semantic_Mapping_via_Sparse_Gaussian_Process_Regression.pdf:pdf},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {6180--6187},
title = {{Dense incremental metric-semantic mapping via sparse gaussian process regression}},
year = {2020}
}
@article{Paz2020a,
abstract = {Recent advancements in statistical learning and computational abilities have enabled autonomous vehicle technology to develop at a much faster rate. While many of the architectures previously introduced are capable of operating under highly dynamic environments, many of these are constrained to smaller-scale deployments, require constant maintenance due to the associated scalability cost with highdefinition (HD) maps, and involve tedious manual labeling. As an attempt to tackle this problem, we propose to fuse image and pre-built point cloud map information to perform automatic and accurate labeling of static landmarks such as roads, sidewalks, crosswalks, and lanes. The method performs semantic segmentation on 2D images, associates the semantic labels with point cloud maps to accurately localize them in the world, and leverages the confusion matrix formulation to construct a probabilistic semantic map in bird's eye view from semantic point clouds. Experiments from data collected in an urban environment show that this model is able to predict most road features and can be extended for automatically incorporating road features into HD maps with potential future work directions.},
archivePrefix = {arXiv},
arxivId = {2006.04894},
author = {Paz, David and Zhang, Hengyuan and Li, Qinru and Xiang, Hao and Christensen, Henrik I.},
doi = {10.1109/IROS45743.2020.9341738},
eprint = {2006.04894},
file = {:home/gian/Documents/Lonomy/ST/Literature/Probabilistic Semantic Mapping for Urban Autonomous Driving.pdf:pdf},
isbn = {9781728162126},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2059--2064},
title = {{Probabilistic semantic mapping for urban autonomous driving applications}},
year = {2020}
}
@article{Cartillier2021a,
abstract = {We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map ('what is where?') from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present Semantic MapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length × width × feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01 − 16.81% (absolute) on mean-IoU and 3.81 − 19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations built by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour ('Find chair') or answering questions about the space ('How many chairs did you see in the house?'). Project page: https://vincentcartillier.github.io/smnet.html.},
archivePrefix = {arXiv},
arxivId = {2010.01191},
author = {Cartillier, Vincent and Ren, Zhile and Jain, Neha and Lee, Stefan and Essa, Irfan and Batra, Dhruv},
eprint = {2010.01191},
file = {:home/gian/Documents/Lonomy/ST/Literature/Semantic MapNet\: Building Allocentric Semanti Maps and Representations from Egocentric Views.pdf:pdf},
isbn = {9781713835974},
journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
pages = {964--972},
title = {{Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views}},
volume = {2A},
year = {2021}
}
@article{Gan2021a,
abstract = {This paper presents a novel and flexible multi-task multi-layer Bayesian mapping framework with readily extendable attribute layers. The proposed framework goes beyond modern metric-semantic maps to provide even richer environmental information for robots in a single mapping formalism while exploiting existing inter-layer correlations. It removes the need for a robot to access and process information from many separate maps when performing a complex task and benefits from the correlation between map layers, advancing the way robots interact with their environments. To this end, we design a multi-task deep neural network with attention mechanisms as our front-end to provide multiple observations for multiple map layers simultaneously. Our back-end runs a scalable closed-form Bayesian inference with only logarithmic time complexity. We apply the framework to build a dense robotic map including metric-semantic occupancy and traversability layers. Traversability ground truth labels are automatically generated from exteroceptive sensory data in a self-supervised manner. We present extensive experimental results on publicly available data sets and data collected by a 3D bipedal robot platform on the University of Michigan North Campus and show reliable mapping performance in different environments. Finally, we also discuss how the current framework can be extended to incorporate more information such as friction, signal strength, temperature, and physical quantity concentration using Gaussian map layers. The software for reproducing the presented results or running on customized data is made publicly available.},
archivePrefix = {arXiv},
arxivId = {2106.14986},
author = {Gan, Lu and Kim, Youngji and Grizzle, Jessy W. and Walls, Jeffrey M. and Kim, Ayoung and Eustice, Ryan M. and Ghaffari, Maani},
eprint = {2106.14986},
file = {:home/gian/Documents/Lonomy/ST/Literature/Multi-Task Learning for Scalable and Dense.pdf:pdf},
pages = {1--16},
title = {{Multi-Task Learning for Scalable and Dense Multi-Layer Bayesian Map Inference}},
url = {http://arxiv.org/abs/2106.14986},
year = {2021}
}
@article{Kochanov2016a,
abstract = {Scene understanding is an important prerequisite for vehicles and robots that operate autonomously in dynamic urban street scenes. For navigation and high-level behavior planning, the robots not only require a persistent 3D model of the static surroundings-equally important, they need to perceive and keep track of dynamic objects. In this paper, we propose a method that incrementally fuses stereo frame observations into temporally consistent semantic 3D maps. In contrast to previous work, our approach uses scene flow to propagate dynamic objects within the map. Our method provides a persistent 3D occupancy as well as semantic belief on static as well as moving objects. This allows for advanced reasoning on objects despite noisy single-frame observations and occlusions. We develop a novel approach to discover object instances based on the temporally consistent shape, appearance, motion, and semantic cues in our maps. We evaluate our approaches to dynamic semantic mapping and object discovery on the popular KITTI benchmark and demonstrate improved results compared to single-frame methods.},
author = {Kochanov, Deyvid and O{\v{s}}ep, Aljo{\v{s}}a and St{\"{u}}ckler, J{\"{o}}rg and Leibe, Bastian},
doi = {10.1109/IROS.2016.7759285},
file = {:home/gian/Documents/Lonomy/ST/Literature/Scene Flow Propagation for Semantic Mapping and Object Discovery in.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {3},
pages = {1785--1792},
title = {{Scene flow propagation for semantic mapping and object discovery in dynamic street scenes}},
volume = {2016-Novem},
year = {2016}
}
@article{Fankhauser2014,
abstract = {This paper addresses the local terrain mapping process for an autonomous robot. Building upon an onboard range measurement sensor and an existing robot pose estimation, we formulate a novel elevation mapping method from a robot-centric perspective. This formulation can explicitly handle drift of the robot pose estimation which occurs for many autonomous robots. Our mapping approach fully incorporates the distance sensor measurement uncertainties and the six-dimensional pose covariance of the robot. We introduce a computationally efficient formulation of the map fusion process, which allows for mapping a terrain at high update rates. Finally, our approach is demonstrated on a quadrupedal robot walking over obstacles.},
author = {Fankhauser, P. and Bloesch, M. and Gehring, C. and Hutter, M. and Siegwart, R.},
doi = {10.1142/9789814623353_0051},
file = {:home/gian/Documents/Lonomy/ST/Robot-centric elevation mapping with uncertainty estimates_2014.pdf:pdf},
isbn = {9789814623346},
journal = {Mobile Service Robotics: Proceedings of the 17th International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines},
keywords = {Elevation mapping,Legged robotics,Terrain estimation},
pages = {433--440},
title = {{Robot-centric elevation mapping with uncertainty estimates}},
year = {2014}
}
@article{Rosinol2020,
abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
archivePrefix = {arXiv},
arxivId = {1910.02490},
author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
doi = {10.1109/ICRA40945.2020.9196885},
eprint = {1910.02490},
file = {:home/gian/Documents/Lonomy/ST/Kimera\: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping:},
isbn = {9781728173955},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1689--1696},
title = {{Kimera: An Open-Source Library for Real-Time Metric-Semantic Localization and Mapping}},
year = {2020}
}
@article{Zhao2019a,
abstract = {Probabilistic traversable map plays a critical role for mobile robot in safe and reliable navigation. Different from structured environment, traversable region in unstructured environment such as grass and sidewalk is relatively more complex. Traditional elevation-based traversable map cannot represent such complex environment well. Thus, it may cause navigation failure. To address this limitation, this paper proposes a novel semantic-elevation mapping approach for navigation task. We first build a multi-layer semantic map from continuous semantic segmentation images. Then, this multi-layer semantic map is fused and converted into a probabilistic map by a distance transform approach. Generated semantic probabilistic map is then fused to an elevation map at path planning stage. The proposed approach is tested on an Unmanned Ground Vehicle (UGV) platform. The results show that our semantic-elevation mapping approach works more reliably and safely than that only with elevation-based approach.},
author = {Zhao, Yimo and Liu, Peilin and Xue, Wuyang and Miao, Ruihang and Gong, Zheng and Ying, Rendong},
doi = {10.1109/ROBIO49542.2019.8961533},
file = {:home/gian/Documents/Lonomy/ST/Semantic_Probabilistic_Traversable_Map_Generation_For_Robot_Path_Planning.pdf:pdf},
isbn = {9781728163215},
journal = {IEEE International Conference on Robotics and Biomimetics, ROBIO 2019},
keywords = {Grid map,LiDAR pointcloud,Probabilistic traversable map,Semantic segmentation,Sensor fusion},
number = {December},
pages = {2576--2582},
title = {{Semantic probabilistic traversable map generation for robot path planning}},
year = {2019}
}
